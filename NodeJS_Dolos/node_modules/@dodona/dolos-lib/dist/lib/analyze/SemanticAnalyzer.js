"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.SemanticAnalyzer = void 0;
const defaultMap_1 = require("../util/defaultMap");
const region_1 = require("../util/region");
const utils_1 = require("../util/utils");
class SemanticAnalyzer {
    constructor(index) {
        this.index = index;
        // How many of your children must match before this node will be matched?
        // For this value, it's not required that your own token excluding children must match
        this.PERCENTAGE_CHILDREN_MATCH_EXCLUSIVE = 0.9;
        // In addition to your own tokens matching, how many of your children's tokens must match?
        this.PERCENTAGE_CHILDREN_MATCH_INCLUSIVE = 0.8;
        // How many strings of your AST node must match before you consider yourself matched?
        // (note: generally there is only one string per AST node, which means this value is in effect 100% most of the time)
        this.PERCENTAGE_OWNNODE_MATCH = 0.8;
    }
    async semanticAnalysis(tokenizedFiles, hashFilter) {
        const results = new Map();
        const astMap = await this.astWithMatches(tokenizedFiles, hashFilter);
        for (const tokenizedFile of tokenizedFiles) {
            results.set(tokenizedFile.id, this.semanticAnalysisOneFile(tokenizedFile, astMap.get(tokenizedFile)));
        }
        return [astMap.get(tokenizedFiles[0]).groups, results];
    }
    semanticAnalysisOneFile(tokenizedFile, matchedAST) {
        const last = this.recurse(tokenizedFile.id, 0, tokenizedFile.ast, matchedAST, 0);
        const matchedLevels = last.lastMatchedLevels;
        if (!matchedLevels)
            return new Map();
        const filterNodeList = (list) => list.filter(p => p.ownNodes.length + p.childrenTotal > 15);
        return (0, utils_1.mapValues)(filterNodeList, matchedLevels);
    }
    recurse(fileId, i, ast, matchedAST, depth) {
        const ownNodes = [];
        const currentChildren = [];
        /*
        The AST is stored in a list of strings-format, where each child starts with '(' and closes with ')'
        Everything inbetween these braces is part of this node in the tree.
        We will gather all parts of this node in a list, and if we encounter a child (signified by '(') we will
        recursively descend in the tree.
    
        Further analysis of this node will happen after we have parsed this AST (the while loop stops)
        */
        let index = i;
        let currentNode = ast[index];
        // This while loop stops when this entire node has been parsed, including all children
        // Or on an edge case that can sometimes happen at the end of the AST.
        while (currentNode != ")" && index < ast.length) {
            /*
            Case analysis of the node:
            1. this is text, we add it to the 'ownNodes' array, which stores all strings part of this node in the tree
            2. it is the token '(', which signifies that we should recursively parse & analyze a child
            3. it is ')', which means this entire node and all it's children have been parsed & analyzed and we are done
               parsing this node, we can proceed to using the parsed info to analyze the node
             */
            if (currentNode !== "(" && currentNode !== ")") {
                // Handle node
                ownNodes.push(index);
            }
            else if (currentNode == "(") {
                // Parse & analyze child node
                const r = (this.recurse(fileId, index + 1, ast, matchedAST, depth + 1));
                index = r.currentI;
                currentChildren.push(r);
            }
            index += 1;
            currentNode = ast[index];
        }
        const occurrenceGroups = matchedAST.groups;
        // Can this node be found in other files?
        // Is a dict which keeps this value for every file, in the form fileId => occurs in this file or not.
        // Because our node can consist of different strings, this is a number instead of a boolean.
        const ownMatchesCountByFile = (0, utils_1.countByKey)(ownNodes
            .map(n => matchedAST.tokenToGroup.get(n))
            .filter((n) => n !== undefined)
            .map(n => occurrenceGroups[n])
            .flat()
            .map(n => n.file.id)
            .filter(n => n !== fileId));
        // Can our children be found in other files?
        // Simply counts the returned values from the analysis performed by our children
        //  Is a dict which keeps this value for every file, in the form fileId => occurs in this file or not.
        const totalChildrenMatchByFile = currentChildren
            .map(c => (0, utils_1.sumByKey)(c.nodeStats.childrenMatch, c.nodeStats.matchedNodeAmount))
            .reduce(utils_1.sumByKey, new Map());
        const totalChildrenCount = currentChildren.reduce((a, b) => a + b.nodeStats.childrenTotal + b.nodeStats.ownNodes.length, 0);
        const childrenOccurrences = currentChildren.map(c => c.nodeStats.occurrences);
        const currentOccurrences = ownNodes
            .map(n => matchedAST.tokenToGroup.get(n))
            .filter((n) => n !== undefined);
        // This set contains all the occurrences (hashes) that are used as 'proof' of matching
        // Useful to later identify common groups
        const occurrences = new Set([...childrenOccurrences.map(v => [...v]), ...currentOccurrences].flat());
        // This is the preliminary result of our analysis, grouping all relevant information together.
        // We will use this information to decide (per file) whether this node is part of a larger group or not.
        const nodeStats = {
            ownNodes,
            matchedNodeAmount: ownMatchesCountByFile,
            childrenTotal: totalChildrenCount,
            childrenMatch: totalChildrenMatchByFile,
            depth,
            occurrences
        };
        // These are all the files where this node could be part of a group.
        // The files which are not candidate files have no matches in this node or the children,
        // so don't have to be analyzed
        const candidateFiles = [
            ...ownMatchesCountByFile.keys(),
            ...currentChildren.map(c => [...c.lastMatchedLevels.keys()]).flat()
        ];
        const lastMatchedLevels = new Map();
        /*
         * For each of the candidate files, we will now try to reason out whether we want to group this node and
         * the children in a matched group or not. This is the main output of the algorithm.
         */
        for (const candidateFile of candidateFiles) {
            // Decides whether the group should be made or not.
            const thisMatches = this.doesLevelMatch(ownNodes, ownMatchesCountByFile.get(candidateFile) || 0, totalChildrenMatchByFile.get(candidateFile) || 0, totalChildrenCount);
            /*
              There are two options:
              1. We match the node: this means that we consider this node and all its children as a semantic group
                 which is common across the original file and the file in this for loop. The output of the algorithm will be
                 this group.
              2. We don't match the node: this means that we don't consider this node and its children as a group. If the
                 children themselves have formed groups, we will return these as the result of this subtree.
             */
            let returnedGroups;
            if (thisMatches) {
                returnedGroups = [nodeStats];
            }
            else {
                returnedGroups = currentChildren.reduce((prev, ch) => [...prev, ...(ch.lastMatchedLevels.get(candidateFile) || [])], []);
            }
            lastMatchedLevels.set(candidateFile, returnedGroups);
        }
        // The 'nodeStats' POJO is the information which is exposed as the results of this algorithm.
        // The other data in this return field is required for correct execution of the algorithm, but is internal data.
        return {
            currentI: index,
            nodeStats,
            lastMatchedLevels
        };
    }
    //
    /*
    This function encodes the reasoning whether a node matches or not.
     */
    doesLevelMatch(ownNodes, ownMatch, childrenMatch, childrenTotal) {
        if (childrenTotal == 0) {
            return ownMatch === ownNodes.length;
        }
        return childrenMatch > childrenTotal * this.PERCENTAGE_CHILDREN_MATCH_EXCLUSIVE ||
            (childrenMatch > childrenTotal * this.PERCENTAGE_CHILDREN_MATCH_INCLUSIVE &&
                ownMatch > ownNodes.length * this.PERCENTAGE_OWNNODE_MATCH);
    }
    async astWithMatches(tokenizedFiles, hashFilter) {
        const occurrenceMap = await this.index.createMatches(tokenizedFiles, hashFilter);
        const groupedOccurrences = [...occurrenceMap.values()];
        const getDefault = () => ({ tokenToGroup: new Map(), groups: groupedOccurrences });
        const astMap = new defaultMap_1.DefaultMap(getDefault);
        for (let i = 0; i < groupedOccurrences.length; i++) {
            const occurrenceGroup = groupedOccurrences[i];
            if (occurrenceGroup.length > 1)
                for (const occurence of occurrenceGroup) {
                    const matchMap = astMap.get(occurence.file);
                    for (let j = occurence.side.start; j <= occurence.side.stop; j++) {
                        matchMap.tokenToGroup.set(j, i);
                    }
                }
        }
        return astMap;
    }
    static getFullRange(file, match) {
        const ast = file.ast;
        const mapping = file.mapping;
        let currentRegion = mapping[match.ownNodes[0]] ||
            new region_1.Region(0, 0, file.lines.length, file.lines[file.lines.length - 1].length);
        for (const ownNode of match.ownNodes) {
            let i = ownNode;
            let done = false;
            let counter = 1;
            while (i < ast.length && !done) {
                if (ast[i] == ")") {
                    counter -= 1;
                }
                else if (ast[i] == "(") {
                    counter += 1;
                }
                else {
                    currentRegion = region_1.Region.merge(currentRegion, mapping[i]);
                }
                if (counter == 0 && i != ownNode)
                    done = true;
                i += 1;
            }
        }
        return currentRegion;
    }
    static pairMatches(fileLeft, fileRight, leftMatches, rightMatches, occurrenceGroups) {
        const pairs = [];
        const notPaired = [];
        const pairedRightMatches = new Set();
        // We look for a pair to every group of the left files
        for (const leftMatch of leftMatches) {
            let assigned = false;
            for (const rightMatch of rightMatches) {
                const is = (0, utils_1.intersect)(leftMatch.occurrences, rightMatch.occurrences);
                if (is.size > leftMatch.occurrences.size * this.PAIRING_TOLERANCE
                    || is.size > rightMatch.occurrences.size * this.PAIRING_TOLERANCE) {
                    pairs.push({ leftMatch, rightMatch }); // if a corresponding right match is found, we add it to the array
                    assigned = true;
                    pairedRightMatches.add(rightMatch);
                }
            }
            // If we can't find any right group that corresponds to a left group, we push the rest of the assignments as
            // unpaired groups
            if (!assigned) {
                const occs = new Array(...leftMatch.occurrences)
                    .map(n => occurrenceGroups[n])
                    .flat()
                    .filter(o => o === fileRight.id);
                notPaired.push({ leftMatch, occurrences: occs });
            }
        }
        // Any right groups that didn't find a left pair in the previous step get pushed as unpaired right groups.
        for (const rightMatch of rightMatches) {
            if (!pairedRightMatches.has(rightMatch)) {
                const occs = new Array(...rightMatch.occurrences)
                    .map(n => occurrenceGroups[n])
                    .flat()
                    .filter(o => o === fileLeft.id);
                notPaired.push({ rightMatch, occurrences: occs });
            }
        }
        // We return both the pairs and the unpaired groups we found
        return [pairs, notPaired];
    }
}
exports.SemanticAnalyzer = SemanticAnalyzer;
// When comparing two semantic groups, we look at how many occurrences that were used to build this match
// are common between these two semantic groups. If there are many, it's quite likely these groups are plagiarized
// from each other.
// This variable denotes how many should be common to consider these groups linked.
SemanticAnalyzer.PAIRING_TOLERANCE = 0.7;
//# sourceMappingURL=SemanticAnalyzer.js.map